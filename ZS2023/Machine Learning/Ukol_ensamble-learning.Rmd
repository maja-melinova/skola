
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>
## Ãškol 4.1
Nasimulujte z normÃ¡lnÃ­ho normovanÃ©ho rozdÄ›lenÃ­ 70 (vzÃ¡jemnÄ› nezÃ¡vislÃ½ch) hodnot pro
kaÅ¾dou z dvaceti nÃ­Å¾e uvaÅ¾ovanÃ½ch promÄ›nnÃ½ch. S vyuÅ¾itÃ­m tÄ›chto dat pomocÃ­ regresnÃ­ho
vztahu:

$$ğ‘¦_ğ‘– = ğ›½_1 ğ‘¥_{1,i} + ğ›½_2 ğ‘¥_{2,ğ‘–} + ... + ğ›½_{20} x_{20,i} + ğ‘¢_ğ‘–$$
nasimulujte hodnoty zÃ¡vislÃ© promÄ›nnÃ© $ğ‘¦$, kde parametry $ğ›½_1$ aÅ¾ $ğ›½_{10} = 1$ a parametry $ğ›½_{11}$ aÅ¾ $ğ›½_{20} = 0,1$. NÃ¡hodnÃ¡ chyba $u_i$ je normÃ¡lnÄ› rozdÄ›lenÃ¡ se stÅ™ednÃ­ hodnotou 0 a rozptylem 1. Takto pÅ™ipravenÃ¡ data od tohoto okamÅ¾iku povaÅ¾ujte za danÃ¡.

DatovÃ½ soubor rozdÄ›lte na 50 a 20 pozorovÃ¡nÃ­. PrvnÃ­ch 50 pozorovÃ¡nÃ­ pouÅ¾ijete pro trÃ©novÃ¡nÃ­, zbylÃ½ch 20 jako testovacÃ­ mnoÅ¾inu pro odhad stÅ™ednÃ­ ÄtvercovÃ© (predikÄnÃ­) chyby.

a. NauÄte na danÃ½ch datech model metodou gradient boosting. Jako slabÃ© modely pouÅ¾ijte jednoduchou lineÃ¡rnÃ­ regresy s jednou vysvÄ›tlujÃ­cÃ­ promÄ›nnou. PouÅ¾ijte mÃ­ru rychlosti uÄenÃ­ o celikosti 0,1. VyuÅ¾ijte kvadratickou ztrÃ¡tovou funkci.
b. NauÄenÃ½ model pouÅ¾ijte pro zÃ­skÃ¡nÃ­ pÅ™edpovÄ›dÃ­ na testovacÃ­ mnoÅ¾inÄ› a urÄete velikost stÅ™ednÃ­ ÄtvercovÃ© chyby. VÃ½sledky srovnejte s tÄ›mi, kterÃ© jste zÃ­skali v minulÃ©m Ãºkolu pomocÃ­ hÅ™ebenovÃ© regresse a LASSO regrese


<hr>

```{r, warning=F, message=F}
library(gbm)
```


```{r}
data <- as.data.frame(matrix(nrow = 70, ncol = 21))
colnames(data) <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13", "X14", "X15", "X16", "X17", "X18", "X19", "X20", "Y")

for(i in 1:20){
  data[,i] <- rnorm(70)
}

data$Y <- 10*data$X1 + 10*data$X2 + 10*data$X3 + 10*data$X4 + 10*data$X5 + 10*data$X6 + 10*data$X7 + 10*data$X8 + 10*data$X9 + 10*data$X10 + 0.1*data$X11 + 0.1*data$X12 + 0.1*data$X13 + 0.1*data$X14 + 0.1*data$X15 + 0.1*data$X16 + 0.1*data$X17 + 0.1*data$X18 + 0.1*data$X19 + 0.1*data$X20 + rnorm(1)
#data$Rozdeleni <- rep(1:5, 14)
```

```{r}
#RozdÄ›lenÃ­ dat na trÃ©novacÃ­ a testovacÃ­
data_train <- data[1:50,]
data_test <- data[51:70,]
```


### Gradient boosting

```{r, message = F}
gbm_model <- gbm(Y ~ X1, data = data_train, distribution = "gaussian", shrinkage = 0.1, n.trees = 5000)

new_data <- data.frame(X1 = data_test[,1])
predikce_gbm <- predict.gbm(gbm_model, newdata = new_data)

#-----

gbm_model2 <- gbm(Y ~ ., data = data_train, distribution = "gaussian", shrinkage = 0.1, n.trees = 5000)

predikce_gbm2 <- predict.gbm(gbm_model2, newdata = data_test[,-21])
```
```{r}
rezidua <- predikce_gbm - data_test$Y
plot(data_test$Y, type = "l")
lines(predikce_gbm, col = "red")

rezidua2 <- predikce_gbm2 - data_test$Y
plot(data_test$Y, type = "l")
lines(predikce_gbm2, col = "red")

c("MSE" = mean(rezidua^2), "MSE2" = mean(rezidua2^2))
```

<hr>
## Ãškol 4.2

Pokuste se zreplikovat cviÄenÃ­ na Bagging z pÅ™ednÃ¡Å¡ek, viz snÃ­mky 5-8 v prezentaci. PÅ™edpoklÃ¡dejte data:

```{r echo = F, results='asis'}
library(knitr)
data_bagging <- as.data.frame(cbind(
  "xi" = c(0.21, 0.21, 0.09, 0.6, 0.99, 0.75, 0.24, 0.09, 0.23, 0.73),
  "yi" = c(0.93, 0.9, 0.71, -0.46, -0.21, -1.07, 1.06, 0.45, 1.07, -0.97)
))

data_bagging$xi2 <- data_bagging$xi^2
data_bagging$xi3 <- data_bagging$xi^3
data_bagging$xi4 <- data_bagging$xi^4
data_bagging$xi5 <- data_bagging$xi^5
data_bagging$xi6 <- data_bagging$xi^6
data_bagging$xi7 <- data_bagging$xi^7

kable(t(data_bagging[,c(1,2)]))
```


S vyuÅ¾itÃ­m dat odhadnÄ›te regresnÃ­ koeficienty polynomickÃ© regrese

$$ğ‘¦_ğ‘– = ğ›½_0+ ğ›½_1 ğ‘¥_i + ğ›½_2 ğ‘¥_ğ‘–^2 + ... + ğ›½_{7} x_i^7 + ğ‘¢_ğ‘–$$

V grafu zobrazte vyrovnanÃ© hodnoty i predikce pro jednotlivÃ¡ x z intervalu (0,1). PosuÄte, jak zÃ­skanÃ© predikce dopadajÃ­ z pohledu kompromisu mezi zkreslenÃ­m a variabilitou. NÃ¡slednÄ› se pokuste pomocÃ­ metody Bagging variabilitu snÃ­Å¾it. V grafu ukaÅ¾te finÃ¡lnÃ­ predikce zÃ­skanÃ© pomocÃ­ tÃ©to metody a zhodnoÅ¥te, zda jsou blÃ­Å¾e funkci, kterÃ¡ bylake generovÃ¡nÃ­ dat skuteÄnÄ› pouÅ¾ita.

<hr>

```{r}
fceZadani <- function(x){
  return(sin(2*pi*x))
}

plot(data_bagging[,c(1,2)], xlim = c(0,1))
curve(fceZadani(x), from = 0, to = 1, col = "red", add = T)
```

```{r, fig.width=10, fig.height=6}
model_M1 <- lm(yi ~ xi, data_bagging)
model_M3 <- lm(yi ~ xi + xi2 + xi3, data_bagging)
model_M7 <- lm(yi ~ xi + xi2 + xi3 + xi4 + xi5 + xi6 + xi7, data_bagging)

fceM3 <- function(x){
  hodnota <- sum(model_M3$coefficients * c(1, x, x^2, x^3))
  return(hodnota)
}

fceM7 <- function(x){
  hodnota <- sum(model_M3$coefficients * c(1, x, x^2, x^3, x^4, x^5, x^6, x^7))
  return(hodnota)
}

par(mfrow=c(2,2))

plot(data_bagging[,c(1,2)], xlim = c(0, 1), ylim = c(-1.6, 1.2))
curve(fceZadani(x), from = 0, to = 1, col = "red", add = T)
abline(h = mean(data_bagging$yi), col = "darkblue")
text(0.9, 0.9, "M = 0")

plot(data_bagging[,c(1,2)], xlim = c(0, 1), ylim = c(-1.6, 1.2))
curve(fceZadani(x), from = 0, to = 1, col = "red", add = T)
curve(model_M1$coefficients[1] + x * model_M1$coefficients[2], from = 0, to = 1, add = T, col = "darkblue")
text(0.9, 0.9, "M = 1")

plot(data_bagging[,c(1,2)], xlim = c(0, 1), ylim = c(-1.6, 1.2))
curve(fceZadani(x), from = 0, to = 1, col = "red", add = T)
curve(Vectorize(fceM3)(x), from = 0, to = 1, col = "darkblue", add = T)
text(0.9, 0.9, "M = 3")

plot(data_bagging[,c(1,2)], xlim = c(0, 1), ylim = c(-1.6, 1.2))
curve(fceZadani(x), from = 0, to = 1, col = "red", add = T)
curve(Vectorize(fceM7)(x), from = 0, to = 1, col = "darkblue", add = T)
text(0.9, 0.9, "M = 7")
```







