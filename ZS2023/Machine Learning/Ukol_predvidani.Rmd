
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>
## Ãškol 3.1
Nasimulujte z normÃ¡lnÃ­ho normovanÃ©ho rozdÄ›lenÃ­ 70 (vzÃ¡jemnÄ› nezÃ¡vislÃ½ch) hodnot pro
kaÅ¾dou z dvaceti nÃ­Å¾e uvaÅ¾ovanÃ½ch promÄ›nnÃ½ch. S vyuÅ¾itÃ­m tÄ›chto dat pomocÃ­ regresnÃ­ho
vztahu:

$$ğ‘¦_ğ‘– = ğ›½_1 ğ‘¥_{1,i} + ğ›½_2 ğ‘¥_{2,ğ‘–} + ... + ğ›½_{20} x_{20,i} + ğ‘¢_ğ‘–$$
nasimulujte hodnoty zÃ¡vislÃ© promÄ›nnÃ© $ğ‘¦$, kde parametry $ğ›½_1$ aÅ¾ $ğ›½_{10} = 1$ a parametry $ğ›½_{11}$ aÅ¾ $ğ›½_{20} = 0,1$. NÃ¡hodnÃ¡ chyba $u_i$ je normÃ¡lnÄ› rozdÄ›lenÃ¡ se stÅ™ednÃ­ hodnotou 0 a rozptylem 1.

Takto pÅ™ipravenÃ¡ data od tohoto okamÅ¾iku povaÅ¾ujte za danÃ¡.
DatovÃ½ soubor rozdÄ›lte na 50 a 20 pozorovÃ¡nÃ­. PrvnÃ­ch 50 pozorovÃ¡nÃ­ pouÅ¾ijte pro
trÃ©novÃ¡nÃ­ a kÅ™Ã­Å¾ovou validaci, zbylÃ½ch 20 jako testovacÃ­ mnoÅ¾inu pro odhad stÅ™ednÃ­
ÄtvercovÃ© (predikÄnÃ­) chyby. Pro predikci pouÅ¾ijte metodu nejmenÅ¡Ã­ch ÄtvercÅ¯, hÅ™ebenovou
regresi a LASSO regresi.

a. Pro optimÃ¡lnÃ­ volbu hyperparametrÅ¯ pouÅ¾ijte 5-nÃ¡sobnou kÅ™Ã­Å¾ovou validaci, pro niÅ¾
si napiÅ¡te vlastnÃ­ R kÃ³d (tj. nevyuÅ¾Ã­vejte dostupnÃ© funkce pro kÅ™Ã­Å¾ovou validaci
v existujÃ­cÃ­ch R balÃ­ÄcÃ­ch). K zÃ­skÃ¡nÃ­ samotnÃ© pÅ™edpovÄ›di dostupnÃ© R balÃ­Äky vyuÅ¾Ã­t
mÅ¯Å¾ete (viz napÅ™. glmnet, larsâ€¦). Pro uvaÅ¾ovanou mnoÅ¾inu hyperparametrÅ¯ vÅ¾dy uveÄte
velikost CV chyby (CV = cross-validation) a jejÃ­ smÄ›rodatnou odchylku. Vyberte
optimÃ¡lnÃ­ hodnotu hyperparametru.
b. VzÃ¡jemnÄ› srovnejte vÃ½sledky zÃ­skanÃ© pomocÃ­ jednotlivÃ½ch metod.
<hr>

```{r, warning=F, message=F}
library(matlib)
library(glmnet)
```


```{r}
data <- as.data.frame(matrix(nrow = 70, ncol = 21))
colnames(data) <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13", "X14", "X15", "X16", "X17", "X18", "X19", "X20", "Y")

for(i in 1:20){
  data[,i] <- rnorm(70)
}

data$Y <- 10*data$X1 + 10*data$X2 + 10*data$X3 + 10*data$X4 + 10*data$X5 + 10*data$X6 + 10*data$X7 + 10*data$X8 + 10*data$X9 + 10*data$X10 + 0.1*data$X11 + 0.1*data$X12 + 0.1*data$X13 + 0.1*data$X14 + 0.1*data$X15 + 0.1*data$X16 + 0.1*data$X17 + 0.1*data$X18 + 0.1*data$X19 + 0.1*data$X20 + rnorm(1)
data$Rozdeleni <- rep(1:5, 14)
```

```{r}
#RozdÄ›lenÃ­ dat na trÃ©novacÃ­ a testovacÃ­
data_train <- data[1:50,]
data_test <- data[51:70,]

X <- cbind(1, as.matrix(data_train[,1:20]))
y <- as.matrix(data_train[,21])
```

```{r}
#Funkce pro vÃ½poÄet stÅ™ednÃ­ ÄtvercovÃ© chyby

MSE <- function(beta, X, y){
  n <- nrow(X)
  y_predikce <- as.matrix(X) %*% beta
 
  MSE <- 1/n * sum((y_predikce - y)^2)
  return(MSE)
}
```


<br>

### Metoda nejmenÅ¡Ã­ch ÄtvercÅ¯


```{r}
beta_OLS <- as.vector(inv(t(X) %*% X) %*% t(X) %*% y)
nazvy <- c()

for(i in 1:21){
  nazvy[i] <- paste("b", (i-1), sep = "")
}

names(beta_OLS) <- nazvy
beta_OLS
```

```{r}
MSE_OLS <- MSE(beta_OLS, cbind(1,data_test[,1:20]), data_test[,21])
```


<br>

### HÅ™ebenovÃ¡ regrese


```{r}
#Volba lambdy
lambdaCV_hreb <- as.data.frame(matrix(nrow = 5, ncol = 2))
colnames(lambdaCV_hreb) = c("Best_lambda", "Best_error")

for(i in 1:5){
  data_test_l <- data_train[data_train$Rozdeleni == i,]
  data_train_l <- data_train[data_train$Rozdeleni != i,]
  
  lambda_values_hreb <- 10^seq(10, -2, length = 200)
  best_error_hreb <- Inf
  best_lambda_hreb <- NULL
  
  for(lambda_hreb in lambda_values_hreb){
    model_hreb <- glmnet(as.matrix(data_train_l[,1:20]), y = data_train_l[,21], aplha = 0, lambda = lambda_hreb)
    
    beta_hreb_lambda <- as.vector(coef(model_hreb))
    X_hreb_lambda <- as.matrix(cbind(1, data_test_l[,1:20]))
    predictions_hreb <- X_hreb_lambda %*% beta_hreb_lambda
    error_hreb <- mean((predictions_hreb - data_test_l[,21])^2)
    
    if(error_hreb < best_error_hreb){
      best_error_hreb <- error_hreb
      best_lambda_hreb <- lambda_hreb
    }
  }
  
  lambdaCV_hreb[i,] <- c(best_lambda_hreb, best_error_hreb)
}

lambda_hreb <- mean(lambdaCV_hreb$Best_lambda)
c("NejlepÅ¡Ã­ lambda" = lambda_hreb, "NejlepÅ¡Ã­ error" = mean(lambdaCV_hreb$Best_error))
```


```{r}
#VÃ½poÄet parametrÅ¯
beta_hreb <- as.vector(inv(t(X) %*% X + diag(lambda_hreb, ncol(X))) %*% t(X) %*% y)

names(beta_hreb) <- nazvy
beta_hreb
```

```{r}
MSE_hreb <- MSE(beta_hreb, cbind(1,data_test[,1:20]), data_test[,21])
```
<br>

### LASSO

```{r}
#Volba lambdy
lambdaCV_lasso <- as.data.frame(matrix(nrow = 5, ncol = 2))
colnames(lambdaCV_lasso) = c("Best_lambda", "Best_error")

for(i in 1:5){
  data_test_l2 <- data_train[data_train$Rozdeleni == i,]
  data_train_l2 <- data_train[data_train$Rozdeleni != i,]
  
  lambda_values_lasso <- 10^seq(10, -2, length = 200)
  best_error_lasso <- Inf
  best_lambda_lasso <- NULL
  
  for(lambda_lasso in lambda_values_lasso){
    model_lasso <- glmnet(as.matrix(data_train_l2[,1:20]), y = data_train_l2[,21], aplha = 1, lambda = lambda_lasso)
    
    beta_lasso_lambda <- as.vector(coef(model_lasso))
    X_lasso_lambda <- as.matrix(cbind(1, data_test_l2[,1:20]))
    predictions_lasso <- X_lasso_lambda %*% beta_lasso_lambda
    error_lasso <- mean((predictions_lasso - data_test_l2[,21])^2)
    
    if(error_lasso < best_error_lasso){
      best_error_lasso <- error_lasso
      best_lambda_lasso <- lambda_lasso
    }
  }
  
  lambdaCV_lasso[i,] <- c(best_lambda_lasso, best_error_lasso)
}

lambda_lasso <- mean(lambdaCV_lasso$Best_lambda)
c("NejlepÅ¡Ã­ lambda" = lambda_lasso, "NejlepÅ¡Ã­ error" = mean(lambdaCV_lasso$Best_error))
```

```{r}

cv <- cv.glmnet(X[,2:21], y, alpha = 1)
mod <- glmnet(X[,2:21], y, alpha = 1, lambda = cv$lambda.min)
bety <- as.vector(coef(mod))
MSE(bety, cbind(1,data_test[,1:20]), data_test[,21])

lasso_model <- glmnet(X[,2:21], y, alpha = 1, lambda = lambda_lasso)
beta_lasso <- as.vector(coef(lasso_model))

names(beta_lasso) <- nazvy
beta_lasso
```

```{r}
MSE_lasso <- MSE(beta_lasso, cbind(1,data_test[,1:20]), data_test[,21])
```
<br>

### PorovnÃ¡nÃ­ ÄtvercovÃ½ch chyb predikce

```{r}
c("MSE - OLS" = MSE_OLS, "MSE - hÅ™ebenovÃ¡ r." = MSE_hreb, "MSE - LASSO" = MSE_lasso)
```






